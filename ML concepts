										HANDS-ON-MACHINE-LEARNING(Aurelien Geron)
									
30-9-20	
Day 1:	
                   Batch Learning - non incremental, restart
                   Online Learing - incremental ,on the fly ,mini batches
								
                               GENERALIZATION  
                   Instance-based Learning - adapting by similarity KNN
                   Model-based Learning - typical LinearRegression
                                   
                   Training Set, Validation Set, Test Set

01-10-20            
Day 2:		  
		   P.Metrics for Regressin - RMSE( outlier sensitive), MAE
		   Metrics for Classificatin - Precision(FP), Recall(FN)
		   f Beta, Accuracy_Score( Equal Distribution of Target Class)
									
		   Feature Normalization - Range of dataset 0 - 1
		   Feature Standardization - Dataset show Gaussian Distribution
								   
		   Gaussian Distribution - Bell Shaped Curve
		   1std = 68%, 2std = 95%, 3std = 99.7%

02-10-20
Day 3:
		   OrdinalImputer, One-Hot-Encoding
		   
		   K-Fold Cross Validation - Checking for Overfitting
		   
		   HypterParameter Tuning - GridSearchCV, RandomizedSearchCV
		   
03-10-20
Day 4:
		   Linear Regression -- The Normal Equation,
		   			Or
		   Gradient Descent - Batch GD, Stochastic SD, Mini-batch GD
		   
04-10-20
Day 5:
		  Polynomial Regression - MOre Feature with extra power( degree )
				Regularized Regression
		  Ridge Regression aka Squared penalty aka L2 norm - @*W**2 -- slope closer to 0
		  Lasso Regression aka Absolute penaly aka L1 norm - @*|W| -- slope could be 0
		  Elastic Net - Sum of Ridge( r=0 ) And Lasso Regression( r == 1 ) 
		  EarlyStopping - Stopping the training as soon as valid error reaches min
		  
05-10-20
Day 6:	
		 Logistic Regression - Sigmoid Functin(h0) for probability

06-10-20
Day 7:	
		 Support Vector Classifier - Marginal Lines Hyperplane Marginal Lines
		 Hard Margin Classifier - follow margin violation( no misclassification ) low bias, high variance
		 SOft Margin Classifier - missclassifiaction allowed ( low varaince, high bias )
		 Support Vecotr Machines - Kernal( NON LINEAR )( classify by transforming 1D --> 2D ), ex polynomial( + features ), rbf, sigmoid
		 Cost Function 1/2*W*W + {C (SUM ( zeta )) for allowing misclassification }
		 Low Weights - high Margin - Less variance, Less Bias
		
07-10-20
 Day 8:
   
		Population, Sample, Variance, Std Devaition
                Decision Tree - Criterion( IMPURITY ) - GINI, ENTROPY
                GINI - 1 - p(+)**2 - p(-)**2  --->>> Gini Gain
                Entropy - -p*logp ---->>>>> Information Gain
             
