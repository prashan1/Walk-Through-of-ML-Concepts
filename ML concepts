										HANDS-ON-MACHINE-LEARNING(Aurelien Geron)
									
30-9-20	
Day 1:	
                   Batch Learning - non incremental, restart
                   Online Learing - incremental ,on the fly ,mini batches
								
                               GENERALIZATION  
                   Instance-based Learning - adapting by similarity KNN
                   Model-based Learning - typical LinearRegression
                                   
                   Training Set, Validation Set, Test Set

01-10-20            
Day 2:		  
		   P.Metrics for Regressin - RMSE( outlier sensitive), MAE
		   Metrics for Classificatin - Precision(FP), Recall(FN)
		   f Beta, Accuracy_Score( Equal Distribution of Target Class)
									
		   Feature Normalization - Range of dataset 0 - 1
		   Feature Standardization - Dataset show Gaussian Distribution
								   
		   Gaussian Distribution - Bell Shaped Curve
		   1std = 68%, 2std = 95%, 3std = 99.7%

02-10-20
Day 3:
		   OrdinalImputer, One-Hot-Encoding
		   
		   K-Fold Cross Validation - Checking for Overfitting
		   
		   HypterParameter Tuning - GridSearchCV, RandomizedSearchCV
		   
03-10-20
Day 4:
		   Linear Regression -- The Normal Equation,
		   			Or
		   Gradient Descent - Batch GD, Stochastic SD, Mini-batch GD
		   
04-10-20
Day 5:
		  Polynomial Regression - MOre Feature with extra power( degree )
				Regularized Regression
		  Ridge Regression aka Squared penalty aka L2 norm - @*W**2 -- slope closer to 0
		  Lasso Regression aka Absolute penaly aka L1 norm - @*|W| -- slope could be 0
		  Elastic Net - Sum of Ridge( r=0 ) And Lasso Regression( r == 1 ) 
		  EarlyStopping - Stopping the training as soon as valid error reaches min
		  
05-10-20
Day 6:	
		 Logistic Regression - Sigmoid Functin(h0) for probability
                 Penalty = Ridge(L2) , Lasso (L1)
06-10-20
Day 7:	
		 Support Vector Classifier - Marginal Lines Hyperplane Marginal Lines
		 Hard Margin Classifier - follow margin violation( no misclassification ) low bias, high variance
		 SOft Margin Classifier - missclassifiaction allowed ( low varaince, high bias )
		 Support Vecotr Machines - Kernal( NON LINEAR )( classify by transforming 1D --> 2D ), ex polynomial( + features ), rbf, sigmoid
		 Cost Function 1/2*W*W + {C (SUM ( zeta )) for allowing misclassification }
		 Low Weights - high Margin - Less variance, Less Bias
		
07-10-20
 Day 8:
                Population, Sample, Variance, Std Devaition
                Decision Tree - Criterion( IMPURITY ) - GINI, ENTROPY
                GINI - 1 - p(+)**2 - p(-)**2  --->>> Gini Gain( gini index - (GI(left)*N + GI(right)*N )
                Entropy - -p*logp ---->>>>> Information Gain( entropy - ||| )
		( InfomrationGain, Gini Gain ) should be high == loww gini inidex or entropy == low impurity === ( High Impurity Removed )
		
08-10-20
Day 9:
					    Ensembling
		Voting Classifier -->> Hard Voting Classifier( ex. majority votes ) Soft Voting Classifier( ex. Average of Proba )
		Bagging --> Bootstrap Aggretition ( feature and instance sampling with replacemnt i.e high non correlated models i.e trading bias for variance i.e higher perfomance) , majority votes, average
		Pasting --> Sampling without replacemnt ( bootstrap == False )
		Out - Of - Bag Instances --> oob_score = True, for validation
		  Random Forest Classifier/Regressor, Feature_importances
		  Extra-Tree --> Extremely Randomized Trees --> for creation of DTrees random threshold is considred for a feature
		Boosting --> training weak learner sequenctly to create a strong learner by correcting the error of predecissor
		  Adaptive boosting --> Weights of wrong instaces if increased and other's decresed to maximize it's probabilty for creating next sample( with more freq of errors ) --> Hard Voting 
		    For Multiclass ( algorithm == SAMME.R )
	          Gradient Boosting Classifer/REgressor -->> Tries to fit the RESIDUAL ERROR OF PREDECESSOR with the new instances 
		Stacking --> Predictions of Weak Learner is samples for the Strong learner( Blender )

09-10-20
Day 10:

		PCA -->> Principal Component Analysis ( Fix Curse of Dimensionality )
		  Projecting our data in PC with minimumn projection loss for preserving maximum variance
		  PC are obtained from SINGULAR VALUE DECOMPOSITION( svd ) == U ( matrix of components )
		  projecting = X * U ( : 2 ) by Explained Variance Ratio
		  Elbow Method or n_estimator = 0.95 ( 95% preserving variance )
		  t-SNE --> t Distributed Stoichastic Neighbour Embedding  FOR VISUALIZING
		  
10-10-20
Day 11:          
                KNN -->>kth Nearest Kneighbour
                  Majority class anong the kth data points
        
